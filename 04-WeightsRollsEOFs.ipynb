{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3d055aaf",
      "metadata": {
        "id": "3d055aaf"
      },
      "source": [
        "# 04: Weighting, Rolling, and EOFs - More on Time Series in xarray"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a076518",
      "metadata": {
        "id": "4a076518"
      },
      "source": [
        "To construct and analyze time series, we often need to process the data to isolate regions or time scales of interest. This tutorial provides examples for two of the more common processing tasks, area-weighted averaging and rolling means."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "214a9ccb",
      "metadata": {
        "id": "214a9ccb",
        "outputId": "6d2dd5b0-8326-42c1-e630-fcdadb520deb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting colormaps\n",
            "  Downloading colormaps-0.4.2-py3-none-any.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.12/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from gdown) (4.13.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from gdown) (3.20.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.12/dist-packages (from gdown) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from gdown) (4.67.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (2025.11.12)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Downloading colormaps-0.4.2-py3-none-any.whl (727 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m727.9/727.9 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: colormaps\n",
            "Successfully installed colormaps-0.4.2\n"
          ]
        }
      ],
      "source": [
        "!pip install colormaps gdown\n",
        "import numpy as np\n",
        "import xarray as xr\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import colormaps\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5d63126",
      "metadata": {
        "id": "d5d63126"
      },
      "source": [
        "The next step is to load the dataset. This time we use a time series of global sea surface temperatures from the [Centennial in situ Observation-Based Estimates version 2 (COBE2)]() analysis prepared by the [Japan Meteorological Agency](https://www.jma.go.jp/jma/indexe.html). The data are provided on a 1° regular latitude-longitude grid as monthly means covering the period from January 1850 until the end of 2024. To reduce the size of the data file, I have selected only the Atlantic sector between 40°S and 40°N."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1aa2d176",
      "metadata": {
        "id": "1aa2d176"
      },
      "outputs": [],
      "source": [
        "!gdown {'1n9-Tue1cinW5YqiXzRJILOzMHkFlgNnj'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c779e952",
      "metadata": {
        "id": "c779e952"
      },
      "outputs": [],
      "source": [
        "sst = xr.open_dataset('sst_cobe2.atlantic.1x1.1850-2024.1m.nc')['sst']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1db4a5db",
      "metadata": {
        "id": "1db4a5db"
      },
      "outputs": [],
      "source": [
        "sst.mean('time').plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8cfbe12",
      "metadata": {
        "id": "d8cfbe12"
      },
      "source": [
        "To construct time series based on area-mean values, we need to take into account differences in the areas of grid cells. For an xarray [DataArray or Dataset](https://tutorial.xarray.dev/fundamentals/01_datastructures.html) on a regular latitude-longitude grid, this can be accounted for by weighting the data object by the cosine of latitude. [Trigonometric functions](https://numpy.org/doc/stable/reference/routines.math.html) in numpy require input in radians, so we use [numpy.deg2rad()](https://numpy.org/doc/stable/reference/generated/numpy.deg2rad.html#numpy.deg2rad) to convert degrees latitude to radians first.\n",
        "- [Weighted reductions in xarray](https://tutorial.xarray.dev/fundamentals/03.4_weighted.html)\n",
        "\n",
        "Here we select a region in the [tropical North Atlantic](https://psl.noaa.gov/data/timeseries/month/DS/TNA/) commonly used as a climate index: 55°W to 15°W and 5°N to 25°N."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88394b33",
      "metadata": {
        "id": "88394b33"
      },
      "outputs": [],
      "source": [
        "# weighted by cosine of latitude\n",
        "weights = np.cos(np.deg2rad(sst.sel(lat=slice(25,5)).lat))\n",
        "# in this dataset, latitude is ordered north to south\n",
        "sst_tna = sst.sel(lat=slice(25,5), lon=slice(-55,-15)).weighted(weights).mean(['lon', 'lat'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1beda60",
      "metadata": {
        "id": "e1beda60"
      },
      "source": [
        "Note that slices for selecting from xarray data objects need to match the order of the dimension. In the COBE2 dataset, latitude is ordered north to south. You can avoid this by using [isel to select by index](https://tutorial.xarray.dev/fundamentals/02.1_indexing_Basic.html), but in this case you need to know the mapping between index and variable.\n",
        "- [Advanced indexing](https://tutorial.xarray.dev/intermediate/indexing/advanced-indexing.html)\n",
        "- [Boolean indexing](https://tutorial.xarray.dev/intermediate/indexing/boolean-masking-indexing.html)\n",
        "\n",
        "Next, we use [groupby](https://tutorial.xarray.dev/fundamentals/03.2_groupby_with_xarray.html) to calculate the anomaly relative to the pre-industrial climatology, approximated as the 1850-1899 mean annual cycle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b81b6ef",
      "metadata": {
        "id": "3b81b6ef"
      },
      "outputs": [],
      "source": [
        "sst_tna = sst_tna.groupby('time.month') - sst_tna.sel(time=slice('1850','1899')).groupby('time.month').mean('time')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bcdfeb88",
      "metadata": {
        "id": "bcdfeb88"
      },
      "source": [
        "We can filter time series to reduce the noise at shorter timescales or isolate processes that evolve on particular time scales. Climate change evolves on relatively long time scales, so a low pass filter is often helpful for visualizing these changes. The simplest of these is probably already familiar to you: the [rolling average](https://docs.xarray.dev/en/stable/generated/xarray.DataArray.rolling.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c463f547",
      "metadata": {
        "id": "c463f547"
      },
      "outputs": [],
      "source": [
        "# Apply a five-year (60-month) rolling average\n",
        "roll_sst = sst_tna.rolling(time=60, center=True).mean().dropna('time')\n",
        "year = sst_tna['time'].dt.year.values\n",
        "dcde = 1850 + 10*((year - year[0]) / 10).astype(int)\n",
        "dec_sst = sst_tna.assign_coords({'decade': ('time', dcde)})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1a9eb18",
      "metadata": {
        "id": "e1a9eb18"
      },
      "source": [
        "You can also define or adopt special window functions for applying rolling computations.\n",
        "- [Windowed computations](https://tutorial.xarray.dev/fundamentals/03.3_windowed.html)\n",
        "\n",
        "Below we plot the area-weighted area-mean sea surface temperature anomalies in the tropical North Atlantic as (1) annual means sorted by decade, (2) the rolling 12-month mean (thin grey line), and (3) the rolling five-year mean (thick black line). When plotting filtered result, we need to pay attention to alignment of the time series. [Rolling averages](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rolling.html) in xarray default to setting the edges of the dataset to [nan](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dropna.html) (['not-a-number'](https://numpy.org/doc/stable//reference/constants.html)). Two ways to deal with these edge effects are included below, one for each of the rolling time series."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "607d1106",
      "metadata": {
        "id": "607d1106"
      },
      "outputs": [],
      "source": [
        "sns.set_style('white', {\"axes.edgecolor\": \"k\"})\n",
        "\n",
        "# set the index for rolling 5-year mean to remove 2.5 years from the beginning and end (10 years = 1 increment)\n",
        "dndx = np.linspace(-0.25,16.75,2041)\n",
        "# for 12-month rolling mean we achieve the same effect by not dropping the nan values\n",
        "sst_12m = sst_tna.rolling(time=12, center=True).mean()\n",
        "yndx = np.linspace(-0.5,17,2100)\n",
        "\n",
        "# take annual averages of the monthly mean anomalies\n",
        "dyr_sst = dec_sst.groupby('time.year').mean('time').assign_coords({'decade': ('year', dcde[::12])})\n",
        "# [::12] means every 12th value, from beginning to end of the array\n",
        "\n",
        "fig, axs = plt.subplots(figsize=(10,5))\n",
        "# Plotting\n",
        "axs.axhline(0, color='k')\n",
        "clrs = colormaps.managua_r\n",
        "sns.stripplot(ax=axs, x=dyr_sst['decade'].values+5, y=dyr_sst.values, hue=dyr_sst.values, palette=clrs, legend=False,\n",
        "              jitter=False, s=15, marker='o', linewidth=1, alpha=0.25) # alpha controls transparency\n",
        "axs.plot(dndx, roll_sst.values, color='#333', lw=2, zorder=10)\n",
        "# this index removes 0.5 year from the beginning and end\n",
        "axs.plot(yndx, sst_12m, color='#999', lw=0.8, zorder=2)\n",
        "axs.set_ylim((-0.8, 1.9))\n",
        "axs.set_xlim((-0.5,17.5))\n",
        "axs.set_xticks(np.arange(-0.5,17))\n",
        "axs.set_xticklabels(np.arange(1850,2021,10).astype('str'))\n",
        "axs.set_ylabel('Tropical North Atlantic SST anomaly [°C]')\n",
        "axs.set_xlabel('Decade')\n",
        "axs.text(0, 1, 'Relative to 1850-1899 mean', ha='left', va='bottom', transform=axs.transAxes)\n",
        "for yy in np.arange(-0.5,17):\n",
        "    axs.axvline(yy, color='#999', ls=':', lw=0.5)\n",
        "for tt in axs.yaxis.get_majorticklocs():\n",
        "    axs.axhline(tt, color='#999', ls=':', lw=0.5)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98753447",
      "metadata": {
        "id": "98753447"
      },
      "source": [
        "One of the issues you may encounter in plotting time series is inconsistencies in how dates are dealt with in different modules. In this example, we have avoided the problem by using [seaborn.stripplot()](https://seaborn.pydata.org/generated/seaborn.stripplot.html) first. This sets the categories along the *x*-axis at unit intervals, each corresponding to one decade. Other approaches:\n",
        "- [How matplotlib handles dates](https://matplotlib.org/stable/users/explain/axes/axes_units.html)\n",
        "- [Axes and cftime: nc-time-axis](https://nc-time-axis.readthedocs.io/en/latest/)\n",
        "\n",
        "For climate indices like the TNA, we often want to remove the long-term trend. Looking at the time series above (and in the previous tutorial for Dakar), a linear trend line is likely not the best option. The most straightforward option is to use a higher-order polynomial fit, such as quadratic (2nd-order) or quartic (4th-order). Again, xarray makes this easy.\n",
        "- [DataArray polyfit](https://docs.xarray.dev/en/latest/generated/xarray.DataArray.polyfit.html)\n",
        "- [Comptation: fitting polynomials](https://docs.xarray.dev/en/stable/user-guide/computation.html#fitting-polynomials)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b76042f",
      "metadata": {
        "id": "9b76042f"
      },
      "outputs": [],
      "source": [
        "linr = sst_tna.polyfit(dim='time', deg=1, full=True)\n",
        "qudr = sst_tna.polyfit(dim='time', deg=2, full=True)\n",
        "qurt = sst_tna.polyfit(dim='time', deg=4, full=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb9f3efc",
      "metadata": {
        "id": "fb9f3efc"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(figsize=(10,5))\n",
        "\n",
        "roll_sst.plot(ax=axs, color='#333', lw=2, zorder=10, label='')\n",
        "sst_12m.plot(ax=axs, color='#999', lw=0.8, zorder=2, label='')\n",
        "\n",
        "# Polynomial fits\n",
        "clrs = colormaps.pastel.colors\n",
        "xr.polyval(coord=sst_tna.time, coeffs=linr.polyfit_coefficients).plot(ax=axs, color=clrs[1],\n",
        "                                                                      lw=2, zorder=10, label='linear')\n",
        "xr.polyval(coord=sst_tna.time, coeffs=qudr.polyfit_coefficients).plot(ax=axs, color=clrs[0],\n",
        "                                                                      lw=2, zorder=10, label='quadratic')\n",
        "xr.polyval(coord=sst_tna.time, coeffs=qurt.polyfit_coefficients).plot(ax=axs, color=clrs[2],\n",
        "                                                                      lw=2, zorder=10, label='quartic')\n",
        "\n",
        "axs.legend(loc='upper left', frameon=False)\n",
        "axs.set_ylim((-0.8, 1.9))\n",
        "axs.set_ylabel('Tropical North Atlantic SST anomaly [°C]')\n",
        "axs.set_xlabel('Year')\n",
        "axs.text(0, 1, 'Relative to 1850-1899 mean', ha='left', va='bottom', transform=axs.transAxes)\n",
        "for yy in axs.xaxis.get_majorticklocs()[1:-1]:\n",
        "    axs.axvline(yy, color='#999', ls=':', lw=0.5)\n",
        "for tt in axs.yaxis.get_majorticklocs():\n",
        "    axs.axhline(tt, color='#999', ls=':', lw=0.5)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58cc8797",
      "metadata": {
        "id": "58cc8797"
      },
      "source": [
        "Whereas the linear fit shows too much warming in the early years and too little in later years, the quadratic and quartic fits match the long-term evolution of the TNA time series fairly well. In this case I would likely use the quadratic fit for detrending because it is less sensitive to the endpoints. The lack of trends before 1900 supports our use of the 1850-1899 period as the baseline climatology.\n",
        "\n",
        "A more sophisticated option would be to use extended empirical orthogonal function analysis (also known as multivariate or multichannel singular spectrum analysis). The [xEOFs](https://xeofs.readthedocs.io/en/latest/index.html) package is a robust tool for computing EEOFs and other EOF-related analyses using xarray objects.\n",
        "- [Removing nonlinear trends with EEOF analysis](https://xeofs.readthedocs.io/en/latest/content/user_guide/auto_examples/1single/plot_eeof_trend.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd29206a",
      "metadata": {
        "id": "fd29206a"
      },
      "outputs": [],
      "source": [
        "!pip install xeofs\n",
        "import xeofs as xe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "312d6efe",
      "metadata": {
        "id": "312d6efe"
      },
      "outputs": [],
      "source": [
        "sst_xy = sst #.sel(lat=slice(25,5), lon=slice(-55,-15))\n",
        "sst_xy = sst_xy.groupby('time.month') - sst_xy.sel(time=slice('1850','1899')).groupby('time.month').mean('time')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a9de1f5",
      "metadata": {
        "id": "6a9de1f5"
      },
      "source": [
        "Paraphrasing from the example linked above, we perform an EEOF analysis on the deseasonalized monthly mean data with a relatively large embedding dimension (embedding = 120 months = 10 years) to highlight long-term trends. Reducing the dimensionality of the input data by selecting a suitably large number of principal components (n_pca_modes=50) speeds up the computation considerably (from more than 4 minutes to less than 4 seconds on my machine, for nearly identical results)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8ac2549",
      "metadata": {
        "id": "c8ac2549"
      },
      "outputs": [],
      "source": [
        "eeof = xe.single.ExtendedEOF(n_modes=5, tau=1, embedding=120, n_pca_modes=50)\n",
        "eeof.fit(sst_xy, dim=\"time\")\n",
        "components_ext = eeof.components()\n",
        "scores_ext = eeof.scores()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67e0e47f",
      "metadata": {
        "id": "67e0e47f"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
        "scores_ext.sel(mode=1).plot(ax=ax[0], label='n_pca_modes=50')\n",
        "components_ext.sel(mode=1, embedding=0).plot(ax=ax[1])\n",
        "ax[0].legend(loc='best')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85942c4b",
      "metadata": {
        "id": "85942c4b"
      },
      "source": [
        "This approach is more complicated than the polynomial fit, but also provides more information. In addition to the spatial structure of the trend, we can identify the influences of the relatively active volcanic period from the late 19th century into the early 20th century and the mid-20th century peak in aerosol emissions from North America and Europe in the time series. Note, however, that this approach may confuse multi-decadal variability and trends (perhaps especially for Atlantic SST).\n",
        "\n",
        "Detrending the original data is then done by applying the reverse transform to only the leading mode:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3e2a7b0",
      "metadata": {
        "id": "a3e2a7b0"
      },
      "outputs": [],
      "source": [
        "sst_trends = eeof.inverse_transform(scores_ext.sel(mode=1))\n",
        "sst_detrended = sst_xy - sst_trends"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a27c065",
      "metadata": {
        "id": "1a27c065"
      },
      "source": [
        "We can then apply a more standard EOF analysis to identify leading modes of variability in the Atlantic SST, with the caveat that these modes are statistically derived and are not always physically meaningful. You can think of this distinction as similar to ordinary least squares linear regression on a two-dimensional scatterplot: the linear fit describes the relationship according to a specific optimization (a squared-error loss function), which may or may not have a clear physical explanation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bffc4634",
      "metadata": {
        "id": "bffc4634"
      },
      "outputs": [],
      "source": [
        "eof_model_detrended = xe.single.EOF(n_modes=5)\n",
        "eof_model_detrended.fit(sst_detrended, dim=\"time\")\n",
        "scores_detrended = eof_model_detrended.scores()\n",
        "components_detrended = eof_model_detrended.components()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4febaffb",
      "metadata": {
        "id": "4febaffb"
      },
      "source": [
        "In this case, the first mode evokes [Atlantic Niño](https://www.aoml.noaa.gov/the-atlantic-nino-el-ninos-little-brother/), although it is a bit broader than this pattern is normally characterized. I have applied a three-month rolling mean to the principal component time series."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0b56e0b",
      "metadata": {
        "id": "a0b56e0b"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
        "scores_detrended.sel(mode=1).rolling(time=3, center=True).mean().dropna('time').plot(ax=ax[0])\n",
        "components_detrended.sel(mode=1).plot(ax=ax[1])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15436a8c",
      "metadata": {
        "id": "15436a8c"
      },
      "source": [
        "The second mode corresponds to the [tropical North Atlantic pattern](https://psl.noaa.gov/data/timeseries/month/DS/TNA/) described by the index we calculated above, along with its [mirror image in the Southern Hemisphere](https://www.wmolc.org/contents/index/Climate+Indices):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6a12bc9",
      "metadata": {
        "id": "e6a12bc9"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
        "scores_detrended.sel(mode=2).plot(ax=ax[0])\n",
        "components_detrended.sel(mode=2).plot(ax=ax[1])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc6f4911",
      "metadata": {
        "id": "bc6f4911"
      },
      "source": [
        "To test this, we can evaluate the [Pearson correlation coefficient](https://docs.xarray.dev/en/stable/generated/xarray.corr.html) between the time series of the second principal component and the TNA index after removing the second-order polynomial trend:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b74e51c",
      "metadata": {
        "id": "5b74e51c"
      },
      "outputs": [],
      "source": [
        "qfit = xr.polyval(coord=sst_tna.time, coeffs=qudr.polyfit_coefficients)\n",
        "# using xarray.corr() allows a choice of\n",
        "xr.corr(sst_tna-qfit,scores_detrended.sel(mode=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2bad5823",
      "metadata": {
        "id": "2bad5823"
      },
      "source": [
        "A similar calculation for the first mode and the Atlantic Nino index (this time using SST detrended by the EEOF method):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab0802cb",
      "metadata": {
        "id": "ab0802cb"
      },
      "outputs": [],
      "source": [
        "anwgt = np.cos(np.deg2rad(sst.sel(lat=slice(3,-3)).lat))\n",
        "atl3 = sst_detrended.sel(lat=slice(3,-3), lon=slice(-20,0)).weighted(anwgt).mean(['lon', 'lat'])\n",
        "xr.corr(atl3,scores_detrended.sel(mode=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10938ecf",
      "metadata": {
        "id": "10938ecf"
      },
      "source": [
        "Both correlations are strong and significant, indicating that the EOF modes can be interpreted as describing the Atlantic Niño and TNA/TSA patterns of SST variability. How much of the variance do they explain?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5bb7f6f",
      "metadata": {
        "id": "a5bb7f6f"
      },
      "outputs": [],
      "source": [
        "eof_model_detrended.explained_variance_ratio()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9afb364f",
      "metadata": {
        "id": "9afb364f"
      },
      "source": [
        "Overall, the mode linked to Atlantic Niño explains about 18% of the variance in SST anomalies within 40°S and 40°N and 80°W and 20°E, while the mode linked to TNA/TSA explains about 15% of the variance. By comparison, the trend component identified using EEOF analysis explains about 35%.\n",
        "\n",
        "The remaining modes are centered outside the tropical Atlantic – plot them to see where the largest anomalies are located."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "metpy",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}